{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"XFWVVjXNiPuz","executionInfo":{"status":"ok","timestamp":1669664021711,"user_tz":480,"elapsed":22584,"user":{"displayName":"Jesper Hauch","userId":"05621687789741802434"}},"outputId":"649b86c0-aff2-4f71-d9e1-2a6c9b292cfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#@title mount drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","id":"YV2lKQfmiZOu","executionInfo":{"status":"ok","timestamp":1669664021715,"user_tz":480,"elapsed":27,"user":{"displayName":"Jesper Hauch","userId":"05621687789741802434"}}},"outputs":[],"source":["#@title set up mount symlink\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_project'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs285_project'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LC4hDhR_ibnm","executionInfo":{"status":"ok","timestamp":1669664052815,"user_tz":480,"elapsed":521,"user":{"displayName":"Jesper Hauch","userId":"05621687789741802434"}},"outputId":"01f468ae-061a-48e6-edd0-c1d55bb581f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs285_project\n","fatal: destination path 'Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning' already exists and is not an empty directory.\n","/content/gdrive/My Drive/cs285_project/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning\n"]}],"source":["#@title change cwd\n","%cd $SYM_PATH\n","!git clone https://github.com/jesperhauch/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.git\n","%cd Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning"]},{"cell_type":"code","source":["!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JCivwUDlu5lZ","executionInfo":{"status":"ok","timestamp":1669664069439,"user_tz":480,"elapsed":6315,"user":{"displayName":"Jesper Hauch","userId":"05621687789741802434"}},"outputId":"60576322-0ab8-44e0-fb66-77313e56f637"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: could not read Username for 'https://github.com': No such device or address\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"K99Cf0zCid9J","outputId":"c96a029c-eed0-4397-d47c-b5420a5f0f9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/ku2482/rltorch (from -r requirements.txt (line 5))\n","  Cloning https://github.com/ku2482/rltorch to /tmp/pip-req-build-78z5zy11\n","  Running command git clone -q https://github.com/ku2482/rltorch /tmp/pip-req-build-78z5zy11\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.12.1+cu113)\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.25.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.9.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.2.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 2)) (4.1.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->-r requirements.txt (line 3)) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->-r requirements.txt (line 3)) (4.13.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->-r requirements.txt (line 3)) (1.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->-r requirements.txt (line 3)) (3.10.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (2.14.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.38.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.50.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.19.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (5.2.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 4)) (3.2.2)\n","Building wheels for collected packages: rltorch\n","  Building wheel for rltorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rltorch: filename=rltorch-0.1.1-py3-none-any.whl size=32738 sha256=dee1a7d88e60e6d94a4e6bf784ad8631abe1d01151032950d54b793744339ebd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5wlvj28s/wheels/b1/8c/64/3b1696276a36646a7937d95c47280a02f88cc57b25426cce68\n","Successfully built rltorch\n","Installing collected packages: rltorch\n","Successfully installed rltorch-0.1.1\n","Already up to date.\n","/content/gdrive/My Drive/cs285_project/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning/KUCodebase/code\n"]}],"source":["#@title install requirements and pull latest changes\n","!pip install -r requirements.txt\n","!git pull\n","%cd KUCodebase/code"]},{"cell_type":"markdown","metadata":{"id":"JDzSZ2BhigPQ"},"source":["# Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170,"status":"ok","timestamp":1669569189559,"user":{"displayName":"Jesper Hauch","userId":"05621687789741802434"},"user_tz":480},"id":"JzGmjy6UE-mt","outputId":"f4b00052-0181-451d-aabd-16eca00baf55"},"outputs":[{"name":"stdout","output_type":"stream","text":["/root\n"]}],"source":["%cd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wl2CvzW5ihhU","outputId":"0c193293-7820-4493-eb8e-5096977c2636"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n","/usr/local/lib/python3.7/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n","  import imp\n","episode: 1     episode steps: 200   reward: -754.7\n","episode: 2     episode steps: 200   reward: -1751.5\n","episode: 3     episode steps: 200   reward: -1381.0\n","episode: 4     episode steps: 200   reward: -864.4\n","/content/gdrive/MyDrive/cs285_project/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning/KUCodebase/code/agent.py:394: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  state = torch.FloatTensor(states).to(self.device)\n","mean norm score 0.9999766943330785\n","std norm score 0.482148651317078\n","------------------------------------------------------------\n","Num steps: 1000   reward: -1329.4\n","------------------------------------------------------------\n","episode: 5     episode steps: 200   reward: -974.5\n","episode: 6     episode steps: 200   reward: -974.3\n","episode: 7     episode steps: 200   reward: -1388.0\n","episode: 8     episode steps: 200   reward: -876.0\n","episode: 9     episode steps: 200   reward: -914.6\n","mean norm score 0.9999393442981798\n","std norm score 0.47254060380243795\n","------------------------------------------------------------\n","Num steps: 2000   reward: -1182.6\n","------------------------------------------------------------\n","episode: 10    episode steps: 200   reward: -1479.9\n","episode: 11    episode steps: 200   reward: -901.3\n","episode: 12    episode steps: 200   reward: -1320.2\n","episode: 13    episode steps: 200   reward: -1187.5\n","episode: 14    episode steps: 200   reward: -1181.1\n","mean norm score 0.9999228156110823\n","std norm score 0.4758390498462902\n","------------------------------------------------------------\n","Num steps: 3000   reward: -1187.1\n","------------------------------------------------------------\n","episode: 15    episode steps: 200   reward: -909.4\n","episode: 16    episode steps: 200   reward: -1480.5\n","episode: 17    episode steps: 200   reward: -1661.9\n","episode: 18    episode steps: 200   reward: -1573.0\n","episode: 19    episode steps: 200   reward: -1339.8\n","mean norm score 0.9999538732668133\n","std norm score 0.4688638177762403\n","------------------------------------------------------------\n","Num steps: 4000   reward: -1289.9\n","------------------------------------------------------------\n","episode: 20    episode steps: 200   reward: -1238.8\n","episode: 21    episode steps: 200   reward: -1106.6\n","episode: 22    episode steps: 200   reward: -1804.6\n","episode: 23    episode steps: 200   reward: -1292.3\n","episode: 24    episode steps: 200   reward: -1559.2\n","mean norm score 0.9947477545338359\n","std norm score 0.4920488575153836\n","------------------------------------------------------------\n","Num steps: 5000   reward: -1263.5\n","------------------------------------------------------------\n","episode: 25    episode steps: 200   reward: -1490.1\n","episode: 26    episode steps: 200   reward: -1662.7\n","episode: 27    episode steps: 200   reward: -1079.7\n","episode: 28    episode steps: 200   reward: -620.8\n","episode: 29    episode steps: 200   reward: -760.5\n","mean norm score -0.638387078809824\n","std norm score 1.0082158985689709\n","------------------------------------------------------------\n","Num steps: 6000   reward: -280.5\n","------------------------------------------------------------\n","episode: 30    episode steps: 200   reward: -492.6\n","episode: 31    episode steps: 200   reward: -362.7\n","episode: 32    episode steps: 200   reward: -359.7\n","episode: 33    episode steps: 200   reward: -122.1\n","episode: 34    episode steps: 200   reward: -125.0\n","mean norm score -0.14052791075498164\n","std norm score 0.5711587777886382\n","------------------------------------------------------------\n","Num steps: 7000   reward: -294.3\n","------------------------------------------------------------\n","episode: 35    episode steps: 200   reward: -125.3\n","episode: 36    episode steps: 200   reward: -357.9\n","episode: 37    episode steps: 200   reward: -498.0\n","episode: 38    episode steps: 200   reward: -479.2\n","episode: 39    episode steps: 200   reward: -357.6\n","mean norm score 0.3683925929090304\n","std norm score 0.40073404373542\n","------------------------------------------------------------\n","Num steps: 8000   reward: -229.0\n","------------------------------------------------------------\n","episode: 40    episode steps: 200   reward: -473.2\n","episode: 41    episode steps: 200   reward: -243.4\n","episode: 42    episode steps: 200   reward: -237.9\n","episode: 43    episode steps: 200   reward: -235.8\n","episode: 44    episode steps: 200   reward: -121.3\n","mean norm score 0.6178395349687335\n","std norm score 0.5233522787861101\n","------------------------------------------------------------\n","Num steps: 9000   reward: -192.4\n","------------------------------------------------------------\n","episode: 45    episode steps: 200   reward: -501.9\n","episode: 46    episode steps: 200   reward: -241.9\n","episode: 47    episode steps: 200   reward: -359.7\n","episode: 48    episode steps: 200   reward: -232.2\n","episode: 49    episode steps: 200   reward: -357.4\n","mean norm score 0.6273977522450369\n","std norm score 0.40922799479439637\n","------------------------------------------------------------\n","Num steps: 10000  reward: -182.0\n","------------------------------------------------------------\n","episode: 50    episode steps: 200   reward: -585.4\n","episode: 51    episode steps: 200   reward: -481.1\n","episode: 52    episode steps: 200   reward: -635.6\n","episode: 53    episode steps: 200   reward: -124.7\n","episode: 54    episode steps: 200   reward: -126.0\n","mean norm score 0.36928652484270696\n","std norm score 0.6616760993390627\n","------------------------------------------------------------\n","Num steps: 11000  reward: -206.8\n","------------------------------------------------------------\n","episode: 55    episode steps: 200   reward: -597.1\n","episode: 56    episode steps: 200   reward: -502.3\n","episode: 57    episode steps: 200   reward: -0.5 \n","episode: 58    episode steps: 200   reward: -365.3\n","episode: 59    episode steps: 200   reward: -237.1\n","mean norm score 0.19561549547645765\n","std norm score 0.3510417023311663\n","------------------------------------------------------------\n","Num steps: 12000  reward: -239.9\n","------------------------------------------------------------\n","episode: 60    episode steps: 200   reward: -238.8\n","episode: 61    episode steps: 200   reward: -236.1\n","episode: 62    episode steps: 200   reward: -125.8\n","episode: 63    episode steps: 200   reward: -467.1\n","episode: 64    episode steps: 200   reward: -126.0\n","mean norm score 0.07261439675265932\n","std norm score 0.32785385432342934\n","------------------------------------------------------------\n","Num steps: 13000  reward: -266.3\n","------------------------------------------------------------\n","episode: 65    episode steps: 200   reward: -543.2\n","episode: 66    episode steps: 200   reward: -357.2\n","episode: 67    episode steps: 200   reward: -119.5\n","episode: 68    episode steps: 200   reward: -241.2\n","episode: 69    episode steps: 200   reward: -245.0\n","mean norm score -0.004576917524023257\n","std norm score 0.2715737749998052\n","------------------------------------------------------------\n","Num steps: 14000  reward: -323.7\n","------------------------------------------------------------\n","episode: 70    episode steps: 200   reward: -124.8\n","episode: 71    episode steps: 200   reward: -0.6 \n","episode: 72    episode steps: 200   reward: -241.4\n","episode: 73    episode steps: 200   reward: -125.1\n","episode: 74    episode steps: 200   reward: -120.8\n","mean norm score 0.0724509093141475\n","std norm score 0.2948824148709507\n","------------------------------------------------------------\n","Num steps: 15000  reward: -170.0\n","------------------------------------------------------------\n","episode: 75    episode steps: 200   reward: -362.3\n","episode: 76    episode steps: 200   reward: -119.0\n","episode: 77    episode steps: 200   reward: -121.8\n","episode: 78    episode steps: 200   reward: -125.9\n","episode: 79    episode steps: 200   reward: -488.3\n","mean norm score -0.10428897304336729\n","std norm score 0.3656073025424923\n","------------------------------------------------------------\n","Num steps: 16000  reward: -216.8\n","------------------------------------------------------------\n","episode: 80    episode steps: 200   reward: -363.9\n","episode: 81    episode steps: 200   reward: -123.2\n","episode: 82    episode steps: 200   reward: -0.5 \n","episode: 83    episode steps: 200   reward: -122.6\n","episode: 84    episode steps: 200   reward: -125.9\n","mean norm score -0.0731462876807275\n","std norm score 0.23108041842630409\n","------------------------------------------------------------\n","Num steps: 17000  reward: -282.0\n","------------------------------------------------------------\n","episode: 85    episode steps: 200   reward: -124.0\n","episode: 86    episode steps: 200   reward: -124.5\n","episode: 87    episode steps: 200   reward: -359.5\n","episode: 88    episode steps: 200   reward: -245.1\n","episode: 89    episode steps: 200   reward: -126.5\n","mean norm score -0.06813040885314058\n","std norm score 0.16239392076169404\n","------------------------------------------------------------\n","Num steps: 18000  reward: -251.3\n","------------------------------------------------------------\n","episode: 90    episode steps: 200   reward: -0.4 \n","episode: 91    episode steps: 200   reward: -360.4\n","episode: 92    episode steps: 200   reward: -0.3 \n","episode: 93    episode steps: 200   reward: -121.8\n","episode: 94    episode steps: 200   reward: -121.1\n","mean norm score -0.16399280085258566\n","std norm score 0.4264196730818789\n","------------------------------------------------------------\n","Num steps: 19000  reward: -179.9\n","------------------------------------------------------------\n","episode: 95    episode steps: 200   reward: -125.9\n","episode: 96    episode steps: 200   reward: -123.3\n","episode: 97    episode steps: 200   reward: -236.2\n","episode: 98    episode steps: 200   reward: -235.8\n","episode: 99    episode steps: 200   reward: -486.9\n","mean norm score -0.13069304476127136\n","std norm score 0.26769069874395895\n","------------------------------------------------------------\n","Num steps: 20000  reward: -289.7\n","------------------------------------------------------------\n","episode: 100   episode steps: 200   reward: -124.1\n","episode: 101   episode steps: 200   reward: -583.7\n","episode: 102   episode steps: 200   reward: -121.0\n","episode: 103   episode steps: 200   reward: -120.7\n","episode: 104   episode steps: 200   reward: -360.9\n","mean norm score -0.12931201358555905\n","std norm score 0.1701412051682512\n","------------------------------------------------------------\n","Num steps: 21000  reward: -240.9\n","------------------------------------------------------------\n","episode: 105   episode steps: 200   reward: -586.7\n","episode: 106   episode steps: 200   reward: -123.0\n","episode: 107   episode steps: 200   reward: -600.2\n","episode: 108   episode steps: 200   reward: -241.1\n","episode: 109   episode steps: 200   reward: -0.5 \n","mean norm score -0.13424365375133418\n","std norm score 0.24355484115251388\n","------------------------------------------------------------\n","Num steps: 22000  reward: -249.0\n","------------------------------------------------------------\n","episode: 110   episode steps: 200   reward: -237.8\n","episode: 111   episode steps: 200   reward: -543.1\n","episode: 112   episode steps: 200   reward: -238.4\n","episode: 113   episode steps: 200   reward: -354.5\n","episode: 114   episode steps: 200   reward: -123.1\n","mean norm score -0.1719048618353936\n","std norm score 0.31421344042376126\n","------------------------------------------------------------\n","Num steps: 23000  reward: -279.9\n","------------------------------------------------------------\n","episode: 115   episode steps: 200   reward: -119.7\n","episode: 116   episode steps: 200   reward: -353.8\n","episode: 117   episode steps: 200   reward: -0.8 \n","episode: 118   episode steps: 200   reward: -123.0\n","episode: 119   episode steps: 200   reward: -241.3\n","mean norm score -0.09228431749803664\n","std norm score 0.18086276490374084\n","------------------------------------------------------------\n","Num steps: 24000  reward: -302.6\n","------------------------------------------------------------\n","episode: 120   episode steps: 200   reward: -628.6\n","episode: 121   episode steps: 200   reward: -358.9\n","episode: 122   episode steps: 200   reward: -119.2\n","episode: 123   episode steps: 200   reward: -240.1\n","episode: 124   episode steps: 200   reward: -360.4\n","mean norm score -0.1142521866744663\n","std norm score 0.15500349884002138\n","------------------------------------------------------------\n","Num steps: 25000  reward: -321.7\n","------------------------------------------------------------\n","episode: 125   episode steps: 200   reward: -0.3 \n","episode: 126   episode steps: 200   reward: -243.6\n","episode: 127   episode steps: 200   reward: -240.1\n","episode: 128   episode steps: 200   reward: -360.8\n","episode: 129   episode steps: 200   reward: -125.7\n","mean norm score -0.11273484659749733\n","std norm score 0.14866401921326813\n","------------------------------------------------------------\n","Num steps: 26000  reward: -240.6\n","------------------------------------------------------------\n","episode: 130   episode steps: 200   reward: -118.1\n","episode: 131   episode steps: 200   reward: -121.5\n","episode: 132   episode steps: 200   reward: -494.6\n","episode: 133   episode steps: 200   reward: -119.2\n","episode: 134   episode steps: 200   reward: -489.3\n","mean norm score -0.11280328394663083\n","std norm score 0.10076967740382627\n","------------------------------------------------------------\n","Num steps: 27000  reward: -255.0\n","------------------------------------------------------------\n","episode: 135   episode steps: 200   reward: -125.2\n","episode: 136   episode steps: 200   reward: -236.1\n","episode: 137   episode steps: 200   reward: -563.4\n","episode: 138   episode steps: 200   reward: -123.3\n","episode: 139   episode steps: 200   reward: -125.5\n","mean norm score -0.12127815818477108\n","std norm score 0.1706798959057506\n","------------------------------------------------------------\n","Num steps: 28000  reward: -289.7\n","------------------------------------------------------------\n","episode: 140   episode steps: 200   reward: -127.5\n","episode: 141   episode steps: 200   reward: -0.3 \n","episode: 142   episode steps: 200   reward: -119.8\n","episode: 143   episode steps: 200   reward: -513.1\n","episode: 144   episode steps: 200   reward: -479.2\n","mean norm score -0.15391984103667694\n","std norm score 0.20750990559117852\n","------------------------------------------------------------\n","Num steps: 29000  reward: -215.5\n","------------------------------------------------------------\n","episode: 145   episode steps: 200   reward: -0.3 \n","episode: 146   episode steps: 200   reward: -359.5\n","episode: 147   episode steps: 200   reward: -239.0\n","episode: 148   episode steps: 200   reward: -122.2\n","episode: 149   episode steps: 200   reward: -351.4\n","mean norm score -0.4133206958899378\n","std norm score 0.4713586807965812\n","------------------------------------------------------------\n","Num steps: 30000  reward: -122.7\n","------------------------------------------------------------\n","episode: 150   episode steps: 200   reward: -0.6 \n","episode: 151   episode steps: 200   reward: -483.3\n","episode: 152   episode steps: 200   reward: -485.2\n","episode: 153   episode steps: 200   reward: -236.5\n","episode: 154   episode steps: 200   reward: -1.1 \n","mean norm score -0.19092193484317824\n","std norm score 0.3576481753888591\n","------------------------------------------------------------\n","Num steps: 31000  reward: -230.4\n","------------------------------------------------------------\n","episode: 155   episode steps: 200   reward: -123.6\n","episode: 156   episode steps: 200   reward: -121.1\n","episode: 157   episode steps: 200   reward: -126.6\n","episode: 158   episode steps: 200   reward: -124.0\n","episode: 159   episode steps: 200   reward: -358.1\n","mean norm score -0.2046841129576365\n","std norm score 0.3931540926231589\n","------------------------------------------------------------\n","Num steps: 32000  reward: -217.0\n","------------------------------------------------------------\n","episode: 160   episode steps: 200   reward: -120.9\n","episode: 161   episode steps: 200   reward: -244.6\n","episode: 162   episode steps: 200   reward: -236.1\n","episode: 163   episode steps: 200   reward: -359.0\n","episode: 164   episode steps: 200   reward: -241.7\n","mean norm score -0.19116681620042913\n","std norm score 0.13278116001095622\n","------------------------------------------------------------\n","Num steps: 33000  reward: -180.8\n","------------------------------------------------------------\n","episode: 165   episode steps: 200   reward: -119.9\n","episode: 166   episode steps: 200   reward: -123.3\n","episode: 167   episode steps: 200   reward: -1.3 \n","episode: 168   episode steps: 200   reward: -125.7\n","episode: 169   episode steps: 200   reward: -475.6\n","mean norm score -0.0910736103026566\n","std norm score 0.12791779115965504\n","------------------------------------------------------------\n","Num steps: 34000  reward: -320.8\n","------------------------------------------------------------\n","episode: 170   episode steps: 200   reward: -359.8\n","episode: 171   episode steps: 200   reward: -119.4\n","episode: 172   episode steps: 200   reward: -494.9\n","episode: 173   episode steps: 200   reward: -124.8\n","episode: 174   episode steps: 200   reward: -348.8\n","mean norm score -0.1648854410253998\n","std norm score 0.25550461845829775\n","------------------------------------------------------------\n","Num steps: 35000  reward: -239.0\n","------------------------------------------------------------\n","episode: 175   episode steps: 200   reward: -0.5 \n","episode: 176   episode steps: 200   reward: -0.3 \n","episode: 177   episode steps: 200   reward: -238.8\n","episode: 178   episode steps: 200   reward: -126.5\n","episode: 179   episode steps: 200   reward: -123.3\n","mean norm score -0.11076388592997077\n","std norm score 0.12467434346018055\n","------------------------------------------------------------\n","Num steps: 36000  reward: -345.9\n","------------------------------------------------------------\n","episode: 180   episode steps: 200   reward: -362.4\n","episode: 181   episode steps: 200   reward: -126.1\n","episode: 182   episode steps: 200   reward: -123.7\n","episode: 183   episode steps: 200   reward: -126.4\n","episode: 184   episode steps: 200   reward: -122.9\n","mean norm score -0.12550102798246524\n","std norm score 0.2617644221929222\n","------------------------------------------------------------\n","Num steps: 37000  reward: -299.9\n","------------------------------------------------------------\n","episode: 185   episode steps: 200   reward: -0.8 \n","episode: 186   episode steps: 200   reward: -363.7\n","episode: 187   episode steps: 200   reward: -351.5\n","episode: 188   episode steps: 200   reward: -232.6\n","episode: 189   episode steps: 200   reward: -237.6\n","mean norm score -0.17729262545740487\n","std norm score 0.29849767225113677\n","------------------------------------------------------------\n","Num steps: 38000  reward: -241.0\n","------------------------------------------------------------\n","episode: 190   episode steps: 200   reward: -123.4\n","episode: 191   episode steps: 200   reward: -471.1\n","episode: 192   episode steps: 200   reward: -361.1\n","episode: 193   episode steps: 200   reward: -592.5\n","episode: 194   episode steps: 200   reward: -125.2\n","mean norm score -0.16239240220052906\n","std norm score 0.13738911956243754\n","------------------------------------------------------------\n","Num steps: 39000  reward: -195.2\n","------------------------------------------------------------\n","episode: 195   episode steps: 200   reward: -571.3\n","episode: 196   episode steps: 200   reward: -0.9 \n","episode: 197   episode steps: 200   reward: -472.8\n","episode: 198   episode steps: 200   reward: -238.1\n","episode: 199   episode steps: 200   reward: -496.0\n","mean norm score -0.1571137839821921\n","std norm score 0.23381248393286513\n","------------------------------------------------------------\n","Num steps: 40000  reward: -266.3\n","------------------------------------------------------------\n","episode: 200   episode steps: 200   reward: -247.3\n","episode: 201   episode steps: 200   reward: -120.1\n","episode: 202   episode steps: 200   reward: -121.0\n","episode: 203   episode steps: 200   reward: -126.2\n","episode: 204   episode steps: 200   reward: -237.1\n","mean norm score -0.12528701410498883\n","std norm score 0.15279595724851955\n","------------------------------------------------------------\n","Num steps: 41000  reward: -247.8\n","------------------------------------------------------------\n","episode: 205   episode steps: 200   reward: -475.1\n","episode: 206   episode steps: 200   reward: -0.7 \n","episode: 207   episode steps: 200   reward: -122.2\n","episode: 208   episode steps: 200   reward: -599.0\n","episode: 209   episode steps: 200   reward: -126.3\n","mean norm score -0.12656422029925266\n","std norm score 0.1423981273066113\n","------------------------------------------------------------\n","Num steps: 42000  reward: -280.2\n","------------------------------------------------------------\n","episode: 210   episode steps: 200   reward: -117.6\n","episode: 211   episode steps: 200   reward: -121.2\n","episode: 212   episode steps: 200   reward: -0.2 \n","episode: 213   episode steps: 200   reward: -119.3\n","episode: 214   episode steps: 200   reward: -126.3\n","mean norm score -0.25306186508897777\n","std norm score 0.39802898402290937\n","------------------------------------------------------------\n","Num steps: 43000  reward: -206.6\n","------------------------------------------------------------\n","episode: 215   episode steps: 200   reward: -122.4\n","episode: 216   episode steps: 200   reward: -1.7 \n","episode: 217   episode steps: 200   reward: -242.0\n","episode: 218   episode steps: 200   reward: -473.7\n","episode: 219   episode steps: 200   reward: -125.5\n","mean norm score -0.16701460852341482\n","std norm score 0.17598111583169082\n","------------------------------------------------------------\n","Num steps: 44000  reward: -218.7\n","------------------------------------------------------------\n","episode: 220   episode steps: 200   reward: -348.4\n","episode: 221   episode steps: 200   reward: -473.7\n","episode: 222   episode steps: 200   reward: -474.6\n","episode: 223   episode steps: 200   reward: -354.5\n","episode: 224   episode steps: 200   reward: -238.3\n","mean norm score -0.13913808287032475\n","std norm score 0.17024106845460626\n","------------------------------------------------------------\n","Num steps: 45000  reward: -228.5\n","------------------------------------------------------------\n","episode: 225   episode steps: 200   reward: -537.0\n","episode: 226   episode steps: 200   reward: -0.9 \n","episode: 227   episode steps: 200   reward: -235.1\n","episode: 228   episode steps: 200   reward: -0.4 \n","episode: 229   episode steps: 200   reward: -0.7 \n","mean norm score -0.18143458408948865\n","std norm score 0.30075060774339024\n","------------------------------------------------------------\n","Num steps: 46000  reward: -328.7\n","------------------------------------------------------------\n","episode: 230   episode steps: 200   reward: -120.7\n","episode: 231   episode steps: 200   reward: -356.6\n","episode: 232   episode steps: 200   reward: -118.2\n","episode: 233   episode steps: 200   reward: -354.5\n","episode: 234   episode steps: 200   reward: -125.4\n","mean norm score -0.12461511304154989\n","std norm score 0.20295759082355339\n","------------------------------------------------------------\n","Num steps: 47000  reward: -353.3\n","------------------------------------------------------------\n","episode: 235   episode steps: 200   reward: -125.0\n","episode: 236   episode steps: 200   reward: -126.4\n","episode: 237   episode steps: 200   reward: -349.7\n","episode: 238   episode steps: 200   reward: -241.1\n","episode: 239   episode steps: 200   reward: -346.5\n","mean norm score -0.11962673058953954\n","std norm score 0.17913898532659775\n","------------------------------------------------------------\n","Num steps: 48000  reward: -295.8\n","------------------------------------------------------------\n","episode: 240   episode steps: 200   reward: -122.8\n","episode: 241   episode steps: 200   reward: -363.1\n"]}],"source":["!python main.py -info redq -env Pendulum-v1 -seed 0 -eval_every 1000 -frames 100000 \\\n","-eval_runs 10 -gpu_id 0 -updates_per_step 20 -method redq -target_entropy -1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wimW8Hxnalc0"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}